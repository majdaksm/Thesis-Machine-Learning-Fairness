{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import imblearn\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define the sensitive attribute 'Age Group' or 'SEX '###\n",
    "sen_att = 'SEX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       LIMIT_BAL  SEX  EDUCATION  MARRIAGE  Age Group  PAY_0  PAY_2  PAY_3  \\\n",
      "0          20000    1          2         1          0      2      2     -1   \n",
      "1         120000    1          2         2          0     -1      2      0   \n",
      "2          90000    1          2         2          0      0      0      0   \n",
      "3          50000    1          2         1          0      0      0      0   \n",
      "4          50000    0          2         1          1     -1      0     -1   \n",
      "...          ...  ...        ...       ...        ...    ...    ...    ...   \n",
      "29995     220000    0          3         1          0      0      0      0   \n",
      "29996     150000    0          3         2          0     -1     -1     -1   \n",
      "29997      30000    0          2         2          0      4      3      2   \n",
      "29998      80000    0          3         1          0      1     -1      0   \n",
      "29999      50000    0          2         1          0      0      0      0   \n",
      "\n",
      "       PAY_4  PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
      "0         -1     -2  ...          0          0          0         0       689   \n",
      "1          0      0  ...       3272       3455       3261         0      1000   \n",
      "2          0      0  ...      14331      14948      15549      1518      1500   \n",
      "3          0      0  ...      28314      28959      29547      2000      2019   \n",
      "4          0      0  ...      20940      19146      19131      2000     36681   \n",
      "...      ...    ...  ...        ...        ...        ...       ...       ...   \n",
      "29995      0      0  ...      88004      31237      15980      8500     20000   \n",
      "29996     -1      0  ...       8979       5190          0      1837      3526   \n",
      "29997     -1      0  ...      20878      20582      19357         0         0   \n",
      "29998      0      0  ...      52774      11855      48944     85900      3409   \n",
      "29999      0      0  ...      36535      32428      15313      2078      1800   \n",
      "\n",
      "       PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  Creditworthiness  \n",
      "0             0         0         0         0                 0  \n",
      "1          1000      1000         0      2000                 0  \n",
      "2          1000      1000      1000      5000                 1  \n",
      "3          1200      1100      1069      1000                 1  \n",
      "4         10000      9000       689       679                 1  \n",
      "...         ...       ...       ...       ...               ...  \n",
      "29995      5003      3047      5000      1000                 1  \n",
      "29996      8998       129         0         0                 1  \n",
      "29997     22000      4200      2000      3100                 0  \n",
      "29998      1178      1926     52964      1804                 0  \n",
      "29999      1430      1000      1000      1000                 0  \n",
      "\n",
      "[30000 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "taiwan_data = pd.read_csv('taiwan_data.csv')\n",
    "print(taiwan_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data in a test and train set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = taiwan_data.loc[:, taiwan_data.columns != 'Creditworthiness']\n",
    "y = taiwan_data.loc[:, taiwan_data.columns == 'Creditworthiness']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=0)\n",
    "\n",
    "### remove the sensitive attributes for execution but store them for metrics later ###\n",
    "\n",
    "X_test_sen = X_test.loc[:,X_test.columns == sen_att]\n",
    "X_test = X_test.loc[:, X_test.columns != sen_att]\n",
    "\n",
    "X_train = X_train.loc[:, X_train.columns != sen_att]\n",
    "\n",
    "#apply SMOTE to the trainingset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state=0)\n",
    "\n",
    "columns = X_train.columns\n",
    "os_data_X, os_data_y=os.fit_sample(X_train, y_train)\n",
    "os_data_X = pd.DataFrame(data=os_data_X,columns=columns )\n",
    "os_data_y= pd.DataFrame(data=os_data_y,columns=['Creditworthiness'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os_data_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of oversampled data is  37382\n",
      "37382\n",
      "Number of no approval in oversampled data 18691\n",
      "Number of approval 18691\n",
      "Proportion of no approval data in oversampled data is  0.5\n",
      "Proportion of approval data in oversampled data is  0.5\n"
     ]
    }
   ],
   "source": [
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_data_X))\n",
    "print (len(os_data_y))\n",
    "print(\"Number of no approval in oversampled data\",len(os_data_y[os_data_y['Creditworthiness']==0]))\n",
    "print(\"Number of approval\",len(os_data_y[os_data_y['Creditworthiness']==1]))\n",
    "print(\"Proportion of no approval data in oversampled data is \",len(os_data_y[os_data_y['Creditworthiness']==0])/len(os_data_X))\n",
    "print(\"Proportion of approval data in oversampled data is \",len(os_data_y[os_data_y['Creditworthiness']==1])/len(os_data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 461  866]\n",
      " [ 988 3685]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os_data_y_2 =os_data_y.to_numpy()\n",
    "os_data_y_2 =os_data_y_2.ravel()\n",
    "\n",
    "logit = LogisticRegression(penalty='l2', solver = 'liblinear', max_iter = 5000)\n",
    "#logit = LogisticRegression()\n",
    "logit.fit(os_data_X, os_data_y_2)\n",
    "\n",
    "\n",
    "y_pred =  logit.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.35      0.33      1327\n",
      "           1       0.81      0.79      0.80      4673\n",
      "\n",
      "    accuracy                           0.69      6000\n",
      "   macro avg       0.56      0.57      0.57      6000\n",
      "weighted avg       0.70      0.69      0.70      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{sen_att: 0}]\n",
    "unprivileged_groups = [{sen_att: 1}]\n",
    "cost_constraint = \"fnr\"\n",
    "randseed = 12345679 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add sensitive attribute back for fairness metrics  ###\n",
    "fair_test_df = pd.concat([X_test, X_test_sen], axis=1, join=\"inner\")\n",
    "fair_test_df = pd.concat([fair_test_df, y_test], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.5680\n",
      "Statistical parity difference = 0.0142\n",
      "Disparate impact = 1.0190\n",
      "Average odds difference = -0.0000\n",
      "Equal opportunity difference = 0.0128\n"
     ]
    }
   ],
   "source": [
    "# Metrics functionZ\n",
    "from collections import OrderedDict\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "\n",
    "dataset = StandardDataset(fair_test_df, \n",
    "                          label_name='Creditworthiness', \n",
    "                          favorable_classes=[1], \n",
    "                          protected_attribute_names=[sen_att], \n",
    "                          privileged_classes=[[0]])\n",
    "\n",
    "def fair_metrics(dataset, y_pred, disp = True):\n",
    "    dataset_pred =dataset.copy()\n",
    "    dataset_pred.labels = y_pred\n",
    "        \n",
    "    attr = dataset_pred.protected_attribute_names[0]\n",
    "    \n",
    "    idx = dataset_pred.protected_attribute_names.index(attr)\n",
    "    privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n",
    "    unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n",
    "\n",
    "    classified_metric_pred = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "    metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "    metrics = OrderedDict()\n",
    "    metrics[\"Balanced accuracy\"] = 0.5*(classified_metric_pred.true_positive_rate()+\n",
    "                                             classified_metric_pred.true_negative_rate())\n",
    "    metrics[\"Statistical parity difference\"] = classified_metric_pred.statistical_parity_difference()\n",
    "    metrics[\"Disparate impact\"] = classified_metric_pred.disparate_impact()\n",
    "    metrics[\"Average odds difference\"] = classified_metric_pred.average_odds_difference()\n",
    "    metrics[\"Equal opportunity difference\"] = classified_metric_pred.equal_opportunity_difference()\n",
    "  \n",
    "    if disp:\n",
    "        for k in metrics:\n",
    "            print(\"%s = %.4f\" % (k, metrics[k]))\n",
    "\n",
    "\n",
    "fair_metrics(dataset, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
