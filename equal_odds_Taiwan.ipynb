{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of an odds-equalizing post-processing algorithm for bias mitigiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aif360\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.metrics import utils\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets.multiclass_label_dataset import MulticlassLabelDataset\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "                import load_preproc_data_adult, load_preproc_data_compas\n",
    "\n",
    "\n",
    "import sklearn \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import imblearn\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import math \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "taiwan_data = pd.read_csv('taiwan_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_att = 'SEX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data in a test val and train set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = taiwan_data.loc[:, taiwan_data.columns != 'Creditworthiness']\n",
    "y = taiwan_data.loc[:, taiwan_data.columns == 'Creditworthiness']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply SMOTE to the trainingset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_train.columns\n",
    "X_train_balanced, y_train_balanced=os.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_taiwan_train_df = pd.concat([X_train_balanced, y_train_balanced], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "taiwan_test_df = pd.concat([X_test, y_test], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "dataset_orig_train = aif360.datasets.BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=balanced_taiwan_train_df,\n",
    "    label_names=['Creditworthiness'],\n",
    "    protected_attribute_names=[sen_att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "dataset_orig_test = aif360.datasets.BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=taiwan_test_df,\n",
    "    label_names=['Creditworthiness'],\n",
    "    protected_attribute_names=[sen_att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "dataset_orig = aif360.datasets.BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=taiwan_data,\n",
    "    label_names=['Creditworthiness'],\n",
    "    protected_attribute_names=[sen_att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{sen_att: 1}]\n",
    "unprivileged_groups = [{sen_att: 0}]\n",
    "cost_constraint = \"fnr\"\n",
    "randseed = 12345679 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic Regression ###\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Placeholder for predicted and transformed datasets\n",
    "dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "dataset_new_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "\n",
    "# Logistic regression classifier and predictions for training data\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train)\n",
    "\n",
    "fav_idx = np.where(lmod.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "y_train_pred_prob = lmod.predict_proba(X_train)[:,fav_idx]\n",
    "\n",
    "X_test = scale_orig.transform(dataset_orig_test.features)\n",
    "y_test_pred_prob = lmod.predict_proba(X_test)[:,fav_idx]\n",
    "\n",
    "class_thresh = 0.5\n",
    "dataset_orig_train_pred.scores = y_train_pred_prob.reshape(-1,1)\n",
    "\n",
    "dataset_orig_test_pred.scores = y_test_pred_prob.reshape(-1,1)\n",
    "\n",
    "y_train_pred = np.zeros_like(dataset_orig_train_pred.labels)\n",
    "y_train_pred[y_train_pred_prob >= class_thresh] = dataset_orig_train_pred.favorable_label\n",
    "y_train_pred[~(y_train_pred_prob >= class_thresh)] = dataset_orig_train_pred.unfavorable_label\n",
    "dataset_orig_train_pred.labels = y_train_pred\n",
    "    \n",
    "y_test_pred = np.zeros_like(dataset_orig_test_pred.labels)\n",
    "y_test_pred[y_test_pred_prob >= class_thresh] = dataset_orig_test_pred.favorable_label\n",
    "y_test_pred[~(y_test_pred_prob >= class_thresh)] = dataset_orig_test_pred.unfavorable_label\n",
    "dataset_orig_test_pred.labels = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "from collections import OrderedDict\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "def compute_metrics(dataset_true, dataset_pred, \n",
    "                    unprivileged_groups, privileged_groups,\n",
    "                    disp = True):\n",
    "    \"\"\" Compute the key metrics \"\"\"\n",
    "    classified_metric_pred = ClassificationMetric(dataset_true,\n",
    "                                                 dataset_pred, \n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "    metrics = OrderedDict()\n",
    "    metrics[\"Balanced accuracy\"] = 0.5*(classified_metric_pred.true_positive_rate()+\n",
    "                                             classified_metric_pred.true_negative_rate())\n",
    "    metrics[\"Statistical parity difference\"] = classified_metric_pred.statistical_parity_difference()\n",
    "    metrics[\"Disparate impact\"] = classified_metric_pred.disparate_impact()\n",
    "    metrics[\"Average odds difference\"] = classified_metric_pred.average_odds_difference()\n",
    "    metrics[\"Equal opportunity difference\"] = classified_metric_pred.equal_opportunity_difference()\n",
    "    \n",
    "    if disp:\n",
    "        for k in metrics:\n",
    "            print(\"%s = %.4f\" % (k, metrics[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Equal odds application ###\n",
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric, DatasetMetric\n",
    "from aif360.algorithms.preprocessing import LFR, Reweighing\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification\n",
    "from time import time\n",
    "\n",
    "EOPP = EqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                             unprivileged_groups = unprivileged_groups,\n",
    "                             seed=42)\n",
    "EOPP = EOPP.fit(dataset_orig_test, dataset_orig_test_pred)\n",
    "dataset_transf_test_pred = EOPP.predict(dataset_orig_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6052\n",
      "Statistical parity difference = -0.0122\n",
      "Disparate impact = 0.9781\n",
      "Average odds difference = -0.0144\n",
      "Equal opportunity difference = 0.0105\n"
     ]
    }
   ],
   "source": [
    " metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred, \n",
    "                                      unprivileged_groups, privileged_groups,\n",
    "                                      disp = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 808  519]\n",
      " [1862 2811]] 0.6031666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.30      0.61      0.40      1327\n",
      "         1.0       0.84      0.60      0.70      4673\n",
      "\n",
      "    accuracy                           0.60      6000\n",
      "   macro avg       0.57      0.61      0.55      6000\n",
      "weighted avg       0.72      0.60      0.64      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### extract test labels and prediction label for traditional model evaluation ###\n",
    "\n",
    "y_test = dataset_orig_test.labels.ravel()\n",
    "y_pred = dataset_transf_test_pred.labels.ravel()\n",
    "\n",
    "### obtain performance metrics###\n",
    "\n",
    "matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "accuracy_score = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "print(matrix, accuracy_score)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
